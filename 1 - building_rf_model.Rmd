---
title: "Build a classification model"
author: "Vaadia et al"
output:
  html_document: default
  pdf_document: default
editor: visual
---

::: callout-tip
This script will help you build a random forest model, following the workflow published in Vaadia et al (Figure 2). This script uses example datasets, but you can replace all the files with your own data to build your own classification model based on your ACC data and your behavioral observations.

***Important!***

-   Read the instructions before running each chunk to adjust the script to your own dataset.
:::

## Set up the R session

```{r setup, warning = FALSE, message = FALSE}

library(tidyverse)
library(moments)
library(tidymodels)
library(ranger)
library(parsnip)
library(caret)
library(zoo)

# Load required functions
source("functions.R")

```

## A - Set parameters

In this section, you will set the parameters needed to run the remaining code: the ACC frequency (in Hz), the bout duration (in seconds) and the type of "bout identification" used by your device.

To identify the bouts (`bout_type`), there are a few possible scenarios: 1) the device indicates the start of an ACC bout (`device`- and you also need to identify the column and variables that identify the start of the bout); 2) the ACC is sampling every X minutes, so you can use the time difference between measurements to identify when a new bout started (`time_diff` - and you need to set the `time_threshold`); 3) the ACC is sampling continuously, in which case you can split the continuous data into bouts of X measurements (`cont`). Here, you select the type relevant for your data.

In our case (i.e., Ornitela GPS-ACC devices), the device identifies the start of a new bout in the column `datatype`, indicated by the variable `SEN_ACC_20Hz_START`.

```{r set_parameters}
# ----- Bout identification -----
# The options are: "device" (if the ACC device identifies the start of the bout), "time_diff" (if the ACC is sampled every X minutes), and "cont" (if the device is sampling continously)

# In our example, we set it to "device", but the code for "time_diff" and "cont" is also provided
bout_type <- set_bout_type("device") 


# Set additional parameters related to the bout identification 
if(bout_type == "device"){
  
  column_bout_id <- "datatype" # what is the column name that identifies the start of the bout and end of a bout?
  start_bout_id <- "SEN_ACC_20Hz_START" # what is the variable that identifies the start of the bout?
  
}

if(bout_type == "time_diff"){
  
  time_threshold <- 300 # set here the time difference expected (in seconds)

  }

# ---- ACC Frequency and Bout duration ----- 
bout_duration <- 5 # add here your relevant bout duration, in seconds
acc_frequency <- 20 # add here the frequency of ACC collection, in Hz


```

## B - Pre-processing sequence

### Upload raw ACC and observations data

::: callout-tip
#### ***Important!***

-   If you want to **use your own** **ACC data**, replace the files inside the folder "Data/ACC_training_data/" with your own ACC data.
-   If you want to **use your own** **behavioral observations data**, replace the files inside the folder "Data/Observations_data/" with your own observations.
:::

```{r uploading_data}
# Uploading the raw ACC and observed behaviors datasets
train_raw_acc <- list.files(path = "Data/ACC_training_data/", 
                            pattern = "*.csv", full.names = T) %>%
  map_df(~read.csv(.)) %>%
  mutate(UTC_datetime = as.POSIXct(UTC_datetime, 
                                   format = c("%Y-%m-%d %H:%M:%S"), 
                                   tz = "UTC"))

observed_behs <- list.files(path = "Data/Observations_data/", 
                            pattern = "*.csv", full.names = T) %>%
  map_df(~read.csv(.)) %>%
  mutate(UTC_datetime = as.POSIXct(UTC_datetime, 
                                   format = c("%Y-%m-%d %H:%M:%S"), 
                                   tz = "UTC"))


```

### 2 - Match ACC bouts and behaviors

```{r match_acc_behaviors}
# If relevant, remove any rows that do not contain ACC data (for example, rows that have only GPS data)
train_raw_acc <- subset(train_raw_acc, datatype != "GPS")

# Match ACC and observed behaviors
acc_behs <- left_join(train_raw_acc, observed_behs, 
                      by = c("device_id", "UTC_datetime"))

```

### 3 - Transform the raw ACC into acceleration values

Each device was calibrated before deployment by measuring the acceleration in all six possible orientations: positive X, negative X, positive Y, negative Y, positive Z, negative Z. We calculated a slope (`slope`) and intercept (`int`) for each axis, representing a device-specific instrument error. Using these values, we can calibrate each device and transform the raw ACC into acceleration values.

For devices without specific error values, we used the average error across all measured devices.

::: callout-tip
#### ***Important!***

-   You need to replace the file "example_calibration.csv" with your own tag calibration file!
:::

```{r calibrating_acc}
# Add calibration file 
calibration <- list.files(path = "Data/Calibration_data/", 
                            pattern = "*.csv", full.names = T) %>%
  map_df(~read.csv(.)) 

acc_behs <- left_join(acc_behs, calibration, by = "device_id")

# If there are no calibration values of a tag, add the mean value for all tags
acc_behs <- acc_behs %>% 
  mutate(slopex = ifelse(is.na(slopex), mean(calibration$slopex), slopex),
         intx = ifelse(is.na(intx), mean(calibration$intx), intx),
         slopey = ifelse(is.na(slopey), mean(calibration$slopey), slopey),
         inty = ifelse(is.na(inty), mean(calibration$inty), inty),
         slopez = ifelse(is.na(slopez), mean(calibration$slopez), slopez),
         intz = ifelse(is.na(intz), mean(calibration$intz), intz))

# Transform raw ACC data into acceleration values
acc_behs <- acc_behs %>%
  mutate(acc_x = (acc_x - intx) * slopex,
         acc_y = (acc_y - inty) * slopey,
         acc_z = (acc_z - intz) * slopez) %>%
    dplyr::select(-c(intx:slopez))

```

### 4 - Identify distinct ACC bouts

To identify the bouts, there are a few possible scenarios: 1) the device indicates the start of an ACC bout; 2) the ACC is sampling every X minutes, so you can use the time difference between measurements to identify when a new bout started; 3) the ACC is sampling continuously, in which case you can split the continuous data into bouts of X measurements.

::: callout-tip
#### ***Important!***

-   Select the appropriate type of "bout identification" on section 1
-   If your bout_type is "device", change the column name and start identifier from your device in the code below
-   If your bout_type is "time_diff", make sure to set an appropriate time difference on section 1
:::

```{r identify_bouts}

if(bout_type == "device"){
  
  bout_id <- numeric(nrow(acc_behs))
  j = 0

  for(i in 1:nrow(acc_behs)) {
    if(acc_behs[[column_bout_id]][i] == start_bout_id) { # these parameters need to be set in section A.i
      j = j + 1
      }
    bout_id[i] = j
  }

  acc_behs <- acc_behs %>% 
    add_column(bout_id, .before = 1)
}


if(bout_type == "time_diff"){
  
  acc_behs <- acc_behs %>%
    group_by(device_id) %>%
    arrange(UTC_datetime) %>%
    mutate(time_diff = as.numeric(difftime(UTC_datetime,
                                           lag(UTC_datetime),
                                           units = "sec"))) %>%
    ungroup() %>%
    mutate(bout_id = cumsum(ifelse(is.na(time_diff) | time_diff >= time_threshold,
                                   1, 0)))

}

if(bout_type == "cont"){
  
  bout_length <- bout_duration * acc_frequency
  
  acc_behs <- acc_behs %>%
    group_by(device_id) %>%
    arrange(UTC_datetime) %>%
    mutate(time_diff = as.numeric(difftime(UTC_datetime,
                                           lag(UTC_datetime),
                                           units = "sec")),
           temp_bout_id = (row_number() - 1) %/% bout_length + 1) %>%
    ungroup() %>%
    # creates a continuous bout id across the whole dataset, based on the cumulative sum of the changes in the temporary bout id
  
    mutate(bout_id = cumsum(temp_bout_id != lag(temp_bout_id, default = 0))) %>%
    dplyr::select(-temp_bout_id)

    # Making sure that the bouts have continuous data and no gaps (not relevant for the example dataset that had distinct bouts every 300 secs)

  acc_behs %>%
    group_by(bout_id) %>%
    filter(max(time_diff) > 1)

}

# Making sure the behaviors are correctly labelled within the same bout
acc_behs <- acc_behs %>% 
  group_by(bout_id) %>%
  mutate(observed_beh = zoo::na.locf(observed_beh)) %>% 
  ungroup()

```

### 5 - Exclude incomplete ACC bouts

The bout length is a product of the bout duration (in seconds) and the ACC frequency (in Hz). In the example dataset, the ACC was collected at 20Hz for 5sec, so we expect each bout to have 100 rows.

```{r exclude_incomplete_bouts}

bout_length <- bout_duration * acc_frequency

acc_behs <- acc_behs %>%
  add_count(bout_id) %>%
  filter(n == bout_length) %>%
  dplyr::select(-n)

```

### 6 - Extract statistical features for each bout

Here we provide a list of statistical features to describe each bout, that are described in Supplementary Table S1. Other features can be added to this function.

```{r extract_stat_features}
stat_feats <- acc_behs %>%
    group_by(device_id, bout_id) %>%
    summarise(mean_x = mean(acc_x),
              mean_y = mean(acc_y),
              mean_z = mean(acc_z),
              range_x = max(acc_x)-min(acc_x),
              range_y = max(acc_y)-min(acc_y),
              range_z = max(acc_z)-min(acc_z),
              sd_x = sd(acc_x),
              sd_y = sd(acc_y),
              sd_z = sd(acc_z),
              skewness_x = skewness(acc_x),
              skewness_y = skewness(acc_y),
              skewness_z = skewness(acc_z),
              kurtosis_x = kurtosis(acc_x),
              kurtosis_y = kurtosis(acc_y),
              kurtosis_z = kurtosis(acc_z),
              max_x = max(acc_x),
              max_y = max(acc_y),
              max_z = max(acc_z),
              min_x = min(acc_x),
              min_y = min(acc_y),
              min_z = min(acc_z),
              norm_x = sqrt(sum(acc_x^2)),
              norm_y = sqrt(sum(acc_y^2)),
              norm_z = sqrt(sum(acc_z^2)),
              q25_x = quantile(acc_x, probs = 0.25),
              q25_y = quantile(acc_y, probs = 0.25),
              q25_z = quantile(acc_z, probs = 0.25),
              q50_x = quantile(acc_x, probs = 0.50),
              q50_y = quantile(acc_y, probs = 0.50),
              q50_z = quantile(acc_z, probs = 0.50),
              q75_x = quantile(acc_x, probs = 0.75),
              q75_y = quantile(acc_y, probs = 0.75),
              q75_z = quantile(acc_z, probs = 0.75),
              cov_x_y = cov(acc_x, acc_y),
              cov_x_z = cov(acc_x, acc_z), 
              cov_y_z = cov(acc_y, acc_z),
              cor_x_y = cor(acc_x, acc_y),
              cor_x_z = cor(acc_x, acc_z),
              cor_y_z = cor(acc_y, acc_z),
              mean_diff_x_y = mean(acc_x-acc_y),
              mean_diff_x_z = mean(acc_x-acc_z),
              mean_diff_y_z = mean(acc_y-acc_z),
              sd_diff_x_y = sd(acc_x-acc_y),
              sd_diff_x_z = sd(acc_x-acc_z),
              sd_diff_y_z = sd(acc_y-acc_z),
              mean_amplitude_x = mean_amplitude(acc_x),
              mean_amplitude_y = mean_amplitude(acc_y),
              mean_amplitude_z = mean_amplitude(acc_z)) %>%
  ungroup()

```

### 6.1 - Prepare the dataset before the model training sequence

The classification model requires a "wide format" dataset, and the current dataset is in "long format". Here, we transform the dataset and add the statistical features.

```{r long_to_wide}
# Transform from long format to wide format
full_data <- acc_behs %>%
  add_column(idx = rep(1:bout_length, nrow(acc_behs)/100)) %>%
  dplyr::select(bout_id, idx, device_id, acc_x, acc_y, acc_z) %>%
  pivot_wider(names_from = idx, values_from = c(acc_x, acc_y, acc_z)) 

full_data <- left_join(full_data, 
                       acc_behs[, c("bout_id", "device_id", "observed_beh")],
                       by = c("device_id", "bout_id"))

full_data <- left_join(full_data, 
                       stat_feats,
                       by = c("device_id", "bout_id"))

```

## C - Model training sequence

Now we enter the model training sequence. By the end of this section, you will have created your own random forest model.

::: callout-tip
#### ***Important!***

-   **Do not run** the next chunk if you are **using your own dataset** .
-   Run the next chunk only if you are using the example datasets.
:::

```{r upload_training_dataset}

################################################################
###### SKIP THIS LINE IF YOU ARE USING YOUR OWN DATASET ########
################################################################

full_data <- list.files(path = "Data/Full_Griffon_training_data/", 
                            pattern = "*.csv", full.names = T) %>%
  map_df(~read.csv(.)) 

```

### 7 - Split the full dataset into the training and test subsets

Here we divide the full dataset into two subsets of data, one for training (67%) and one for testing (33%). These percentages can be changed within the function `initial_split(prop = 0.667)`.

```{r training_test_subsets}
set.seed(1353)
data_split <- initial_split(full_data, 
                            strata = observed_beh, 
                            prop = 0.667) # change the proportion here

train_data <- training(data_split)
test_data <- testing(data_split)

```

### 8 - Train the random forest

We start by defining the parameters of the random forest:\
`mtry`- number of predictors that are randomly sampled at each split when creating the tree models;\
`min_n`- minimum number of observations in a leaf that are required to split the leaf further;\
`trees`- number of trees in the forest

Then we set the recipe for the model and finally run the model on the training subset.

```{r training_rf}
# Set the parameters of the RF model
rf_mod <- rand_forest(mtry = 10, min_n = 11, trees = 1000) %>% 
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

# Set the recipe for the RF model
rf_recipe <- 
  recipe(observed_beh ~ ., data = train_data) %>% 
  step_rm(device_id, starts_with("acc_"))

# Train the RF model using the "training subset"
training_fit <- workflow() %>%
    add_recipe(rf_recipe) %>%
    add_model(rf_mod) %>%
    fit(train_data) 

```

### 9 - Test the random forest model

Now we test the model, using the "test subset" of the data. We also calculate the confidence score of each prediction.

```{r test_rf}
# Test the RF model using the "testing subset"
predictions <-  predict(training_fit, test_data)

# Calculate a confidence score for each prediction
scores <- predict(training_fit, test_data, type='prob')


```

### 10 - Evaluate the model performance and extract metrics

We calculate a confusion matrix, and with the number of true positives, false positives, true negatives and false negatives, calculate metrics such as accuracy, precision and recall.

```{r confusion_matrix}
test_data_preds <- test_data %>%
  add_column(predicted_beh = predictions$.pred_class, .before = 'device_id') %>%
  add_column(score = apply(scores, MARGIN = 1, FUN = max), .before = 'device_id')

confusion_matrix <- caret::confusionMatrix(
  data = test_data_preds$predicted_beh, 
  reference = as.factor(test_data_preds$observed_beh), 
  mode = "prec_recall")

```

### 13 - Build the final random forest model

After building a model with a good performance, we use the full training dataset to build the final random forest model. We then save the final model under the `final_model_fit.rda`, which will be used in the next script "Using an existing classification model".

```{r build_final_rf}

final_fit <- workflow() %>%
    add_recipe(rf_recipe) %>%
    add_model(rf_mod) %>%
    fit(full_data) 

# Save the final model, which will be used in the following script
saveRDS(final_fit, file = "Data/My_RF_model/final_model_fit.rda")

```
